{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e601bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00124593",
   "metadata": {},
   "source": [
    "در اینجا برای تعریف تابع اینفورمیشن گین، برای سادگی کار در ادامه، از تعریف دو گرهی که هر ویژگی داده ها را تقسیم بندی میکند استفاده میکنیم و تابع اینوفرمیشن گینز را برای ویژگی های مختلف تعریف میکنیم که شامل یک دیکشنری است که به ازای هر ویژگی که در اینجا با توجه به استفاده از آنالیز مولفه های اصلی 10 تاست، تعریف میشود و به ازای هر ویژگی در این دیکشنری یک بهره اطلاعات ذخیره میشود. . ، "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feb993a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    probabilites = counts / len(y)\n",
    "    entropy = -np.sum(probabilites * np.log2(probabilites))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(y, y_left_node, y_right_node):\n",
    "    p_left = len(y_left_node) / len(y)\n",
    "    p_right = len(y_right_node) / len(y)\n",
    "    info_gain = entropy(y) - (p_left * entropy(y_left_node) + p_right * entropy(y_right_node))\n",
    "    return info_gain\n",
    "\n",
    "def information_gains(x, y):\n",
    "    information_gains = {}  \n",
    "    for feature in range(10):\n",
    "        information_gains[feature] = information_gain(y,y_left_node,y_right_node)\n",
    "    return information_gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38efe3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, choice=None, left_node=None, right_node=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.choice = choice\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        \n",
    "        \n",
    "def fit(X, y, max_depth=None):\n",
    "    #در ابتدا شرطی قرار میدهیم که با توجه به حداکثر عمقی که تعریف میکنیم این الگوریتم ادامه پیدا کند . \n",
    "    #شرط بعدی این است که در یک داده ی جداسازی شده اگر تمامی برچسب ها متعلق به یک کلاس بودند، این الگوریتم متوفق میشود. \n",
    "    # با توقف این الگوریتم، شماره ی کلاسی که بیشترین تکرار را دارد در متغیر(choice) ذخیره میشود.   \n",
    "    if max_depth is not None and max_depth == 0 or len(np.unique(y)) == 1:\n",
    "        choice = np.argmax(np.bincount(y))\n",
    "        return Node(choice=choice)\n",
    "    # این متغیرها برای انتخاب بهترین بهره ی اطلاعاتی و بهترین ویژگی و آستانه ی مناسب برای داده ها مقداردهی اولیه میشوند..\n",
    "    best_info_gain = -np.inf\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    \n",
    "    # در این مرحله، از بین 10 ویژگی که در میان دادگان وجود دارد و بین آستانه های مختلف، یک آستانه و ویژگی بهینه برای تقسیم داده ها به دو گره مختلف انتخاب میشوند.\n",
    "    # با بررسی مقدار داده ها بعد از پی سی ای، به این نتیجه میرسیم که داده ها بین -5 و 5 قرار میگیرند و با سرچ بین آستانه های مختلف، میتوانیم آستانه ی بهینه را انتخاب کنیم.\n",
    "    # دلیل وجود این اعداد این است که اگر به صورت پیوسته آستانه سرچ میشد، بار محاسباتی بالایی داشت، اما با انتخاب گسسته بین اعداد صحیح، دقت تفاوت زیادی نمیکند و بار محاسباتی به شدت کاهش می یابد.\n",
    "    \n",
    "    for feature in range(10):\n",
    "        thresholds = [-4,-3,-2,-1,0,1,2,3,4]\n",
    "        for threshold in thresholds:\n",
    "            #  در اینجا به ازای ویژگی ها و آستانه های مختلف، مقادیر داده با آستانه مقایسه میشوندو با توجه به درستی بزرگ یا کوچک بودن این عبارت، برچسب های گره های چپ و راست تشکیل میشوند. \n",
    "            indice = X[:, feature] <= threshold\n",
    "            y_left_node = y[indice]\n",
    "            y_right_node = y[~indice]\n",
    "            inform_gain = information_gain(y, y_left_node, y_right_node)\n",
    "            # در اینجا نیز عملیات مقایسه بین ویژگی های مختلف و آستانه های مختلف برای هر داده برای تشکیل گره با توجه به بهره ی اطلاعاتی انجام میشود.\n",
    "            if inform_gain > best_info_gain:\n",
    "                best_info_gain = inform_gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    # در اینجا هم بعد از تمام شدن حلقه، مقادیر دادگان و برچسب های آنها باهم دیگر آپدیت میشوند. \n",
    "    indice = X[:, best_feature] <= best_threshold\n",
    "    X_left_node, y_left_node = X[indice], y[indice]\n",
    "    X_right_node, y_right_node = X[~indice], y[~indice]\n",
    "    # در اینجا درخت به صورت بازگشتی آموزش میبیند تا زمانی که عمق تعیین شده ی ما به صفر برسد. \n",
    "    left_node = fit(X_left_node, y_left_node, max_depth - 1) if max_depth is not None else fit(X_left_node, y_left_node)\n",
    "    right_node = fit(X_right_node, y_right_node, max_depth - 1) if max_depth is not None else fit(X_right_node, y_right_node)\n",
    "    \n",
    "    return Node(feature=best_feature, threshold=best_threshold, left_node=left_node, right_node=right_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0555e164",
   "metadata": {},
   "source": [
    "در اینجا دادگان را لود میکنیم و ابتدا به 0 و1 نرمالیزه میکنیم. سپس به آرایه با سایز مورد نظر تغییر سایز میدهیم و بعد بر روی داده ی تست و آموزشی، آنالیز مولفه اصلی با 10 مولفه اعمال میکنیم و در نهایت مدل را با تعیین یک عمق حداکثری آموزش می دهیم. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e227287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.load('F:/mnist/x_train.npy')\n",
    "#y_train = np.load('F:/mnist/y_train.npy')\n",
    "#x_test = np.load('F:/mnist/x_test.npy')\n",
    "#y_test = np.load('F:/mnist/y_test.npy')\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bdf78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
    "x_test = (x_test - np.min(x_test)) / (np.max(x_test) - np.min(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d5dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b3d3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69beefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 10\n",
    "model = fit(x_train, y_train, max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65a6c1",
   "metadata": {},
   "source": [
    "در اینجا برای مرحله ی تست، با توجه به ورودی که به تابع پیش بینی میدهیم، بررسی میشود که اگر با توجه به مدل فیت شده و ورودی تست داده شده، اگر برای متغیر انتخاب، ما ورودی داشتیم، متغیر انتخاب به عنوان گره نهایی که داده را طبقه بندی کرده است و پیش بینی نهایی ماست، بازگردانده میشود، در غیر انیصورت باز هم با مقایسه ی با آستانه، به گره های چپ و راست برده میشوند تا پیش بینی به درستی انجام شود.\n",
    "در نهایت هم به تعداد دادگان تست، در لیست پیشبینی، تخمین های مربوط به دادگان تست ذخیره میشوند و در مرحله ی آخر برای محاسبه ی دقت الگوریتم، این لیست پیشبینی با برچسب های دادگان اصلی مقایسه خواهد شد. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbd981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(model, X):\n",
    "    if model.choice is not None:\n",
    "        return model.choice\n",
    "    if X[model.feature] <= model.threshold:\n",
    "        return predict(model.left_node, X)\n",
    "    else:\n",
    "        return predict(model.right_node, X)\n",
    "predictions = []\n",
    "for i in range(len(x_test)):\n",
    "    prediction = predict(model, x_test[i])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78673edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
